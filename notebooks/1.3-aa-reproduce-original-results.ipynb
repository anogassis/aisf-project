{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/anogassis/aisf-project/blob/eda-notebooks/notebooks/1.1-aa-tms-experiments_tassilo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJAFEkSxxV-0"
   },
   "source": [
    "# Toy Models of Superposition\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timaeus-research/devinterp/blob/main/examples/tms.ipynb)\n",
    "\n",
    "Let's run through an example using Anthropic's toy models of superposition.\n",
    "\n",
    "This example is mostly to test that our SGLD estimator is working as expected and to figure out how to integrate this in an SGD setting.\n",
    "\n",
    "Credits: [Chen et al. (2023)](https://arxiv.org/abs/2310.06301)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03mHkk4txV-4"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1oevvlQyKCt"
   },
   "source": [
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3oIgQvnuxV-6",
    "outputId": "d57471fa-8328-4577-a0bb-97a2d443a971",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from typing import Iterable, Optional, Callable, Dict, List, Any\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.spatial import ConvexHull\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.nn import functional as F\n",
    "from devinterp.slt import estimate_learning_coeff_with_summary\n",
    "from devinterp.optim.sgld import SGLD\n",
    "from collections import defaultdict\n",
    "\n",
    "from abc import ABC\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9ZEya3-yVdB"
   },
   "source": [
    "### Toy Models & Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9UIxLuCyM89",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToyAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Network class for linear transformation with non-linear activations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        The dimension of the input data.\n",
    "    hidden_dim : int\n",
    "        The dimension of the hidden layer.\n",
    "    n_instances : int, optional\n",
    "        The number of instances of the network, default is 1.\n",
    "    tied : bool, optional\n",
    "        If True, ties the weights of the embedding and unembedding layers, default is True.\n",
    "    final_bias : bool, optional\n",
    "        If True, includes a bias term in the unembedding layer, default is False.\n",
    "    hidden_bias : bool, optional\n",
    "        If True, includes a bias term in the embedding layer, default is False.\n",
    "    nonlinearity : callable, optional\n",
    "        The non-linear activation function to apply, default is F.relu.\n",
    "    unit_weights : bool, optional\n",
    "        If True, normalizes the weights to have unit norm, default is False.\n",
    "    standard_magnitude : bool, optional\n",
    "        If True, normalizes weights to maintain average norm, default is False.\n",
    "    initial_scale_factor : float, optional\n",
    "        Initial scale factor for weights, default is 1.0.\n",
    "    initial_bias : Optional[torch.Tensor], optional\n",
    "        Initial bias tensor for the unembedding layer, default is None.\n",
    "    initial_embed : Optional[torch.Tensor], optional\n",
    "        Initial embedding tensor for the embedding layer, default is None.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x: torch.Tensor) -> torch.Tensor\n",
    "        Forward pass through the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        n_instances: int = 1,\n",
    "        tied: bool = True,\n",
    "        final_bias: bool = False,\n",
    "        hidden_bias: bool = False,\n",
    "        nonlinearity: Callable = F.relu,\n",
    "        unit_weights: bool = False,\n",
    "        standard_magnitude: bool = False,\n",
    "        initial_scale_factor: float = 1.0,\n",
    "        initial_bias: Optional[torch.Tensor] = None,\n",
    "        initial_embed: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set the dimensions and parameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_instances = n_instances\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.tied = tied\n",
    "        self.final_bias = final_bias\n",
    "        self.unit_weights = unit_weights\n",
    "        self.standard_magnitude = standard_magnitude\n",
    "\n",
    "        # Define the input layer (embedding)\n",
    "        self.embedding = nn.Linear(self.input_dim, self.hidden_dim, bias=hidden_bias)\n",
    "\n",
    "        # Set initial embeddings if provided\n",
    "        if initial_embed is not None:\n",
    "            self.embedding.weight.data = initial_embed\n",
    "\n",
    "        # Define the output layer (unembedding)\n",
    "        self.unembedding = nn.Linear(self.hidden_dim, self.input_dim, bias=final_bias)\n",
    "\n",
    "        # Set initial bias if provided\n",
    "        if initial_bias is not None:\n",
    "            self.unembedding.bias.data = initial_bias\n",
    "\n",
    "        # If standard magnitude is set, normalize weights and maintain average norm\n",
    "        if self.standard_magnitude:\n",
    "            avg_norm = torch.norm(self.embedding.weight.data, p=2, dim=0).mean()\n",
    "            self.embedding.weight.data = (\n",
    "                F.normalize(self.embedding.weight.data, p=2, dim=0) * avg_norm\n",
    "            )\n",
    "\n",
    "        # If unit weights is set, normalize weights\n",
    "        if self.unit_weights:\n",
    "            self.embedding.weight.data = F.normalize(self.embedding.weight.data, p=2, dim=0)\n",
    "\n",
    "        # Tie the weights of embedding and unembedding layers\n",
    "        if tied:\n",
    "            self.unembedding.weight = torch.nn.Parameter(self.embedding.weight.transpose(0, 1))\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor to the network.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor after applying the embedding, unembedding, and nonlinearity.\n",
    "        \"\"\"\n",
    "        # Apply the same steps for weights as done during initialization\n",
    "        if self.unit_weights:\n",
    "            self.embedding.weight.data = F.normalize(self.embedding.weight.data, p=2, dim=0)\n",
    "\n",
    "        if self.standard_magnitude:\n",
    "            avg_norm = torch.norm(self.embedding.weight.data, p=2, dim=0).mean()\n",
    "            self.embedding.weight.data = (\n",
    "                F.normalize(self.embedding.weight.data, p=2, dim=0) * avg_norm\n",
    "            )\n",
    "\n",
    "        if self.tied:\n",
    "            self.unembedding.weight.data = self.embedding.weight.data.transpose(0, 1)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = self.unembedding(x)\n",
    "        x = self.nonlinearity(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOORH8-_ydF4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from [TMS-zoo](https://github.com/JakeMendel/TMS-zoo)\n",
    "\"\"\"\n",
    "\n",
    "class SyntheticDataset(Dataset, ABC):\n",
    "    num_samples: int\n",
    "    num_features: int\n",
    "    sparsity: Union[float, int]\n",
    "    # importance: Optional[float]\n",
    "\n",
    "    \"\"\"\n",
    "    Abstract base class for synthetic datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_samples : int\n",
    "        The number of samples to generate.\n",
    "    num_features : int\n",
    "        The dimension of the feature vector.\n",
    "    sparsity : Union[float, int]\n",
    "        If float, the probability that a given feature is zero. \n",
    "        If int, the number of features that are set to one.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    generate_values() -> torch.Tensor\n",
    "        Abstract method to generate the values for the dataset.\n",
    "    generate_mask() -> torch.Tensor\n",
    "        Generate a sparse mask for the given dataset.\n",
    "    generate_data() -> torch.Tensor\n",
    "        Generate the synthetic data.\n",
    "    __len__() -> int\n",
    "        Return the number of samples in the dataset.\n",
    "    __getitem__(idx: int) -> torch.Tensor\n",
    "        Return the sample at the given index.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_samples,\n",
    "        num_features,\n",
    "        sparsity,\n",
    "        # importance=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the SyntheticDataset object.\n",
    "        \"\"\"\n",
    "        self.num_samples = num_samples  # The number of samples in the dataset\n",
    "        self.num_features = num_features  # The size of the feature vector for each sample\n",
    "        self.sparsity = sparsity\n",
    "        # self.importance = importance\n",
    "        self.data = self.generate_data()  # Generate the synthetic data\n",
    "\n",
    "    def generate_values(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate_mask(self):\n",
    "        \"\"\"\n",
    "        Generate a sparse mask for the given dataset.\n",
    "        If ``sparsity`` is a float, then the mask is generated by sampling from a Bernoulli distribution with parameter ``1 - sparsity``.\n",
    "        If ``sparsity`` is an integer, then the mask is generated by sampling exactly ``sparsity`` indices without replacement.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A sparse mask for the given dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(self.sparsity, float):\n",
    "            return torch.bernoulli(\n",
    "                torch.ones((self.num_samples, self.num_features)) * (1 - self.sparsity)\n",
    "            )\n",
    "        elif isinstance(self.sparsity, int):\n",
    "            mask = torch.zeros((self.num_samples, self.num_features))\n",
    "            for i in range(self.num_samples):\n",
    "                indices = torch.randperm(self.num_features)[: self.sparsity]\n",
    "                mask[i, indices] = 1\n",
    "\n",
    "            return mask\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Sparsity must be a float or an integer. Received {type(self.sparsity)}.\"\n",
    "            )\n",
    "\n",
    "    def generate_data(self):\n",
    "        \"\"\"\n",
    "        Generate the synthetic data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The synthetic data.\n",
    "        \"\"\"\n",
    "        return self.generate_mask() * self.generate_values()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of samples in the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of samples.\n",
    "        \"\"\"\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return the sample at the given index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            The index of the sample to return.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The sample at the given index.\n",
    "        \"\"\"\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class SyntheticUniformValued(SyntheticDataset):\n",
    "    \"\"\"\n",
    "    This class creates a synthetic dataset where each sample is a vector with indices that are zero with probability `sparsity` and uniform between 0 and 1 otherwise.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    generate_values() -> torch.Tensor\n",
    "        Generate the values for the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def generate_values(self):\n",
    "        \"\"\"\n",
    "        Generate the values for the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The values for the dataset, uniform between 0 and 1.\n",
    "        \"\"\"\n",
    "        return torch.rand((self.num_samples, self.num_features))\n",
    "\n",
    "\n",
    "class SyntheticBinaryValued(SyntheticDataset):\n",
    "    \"\"\"\n",
    "    This class creates a synthetic dataset where each sample is a vector with indices that are zero with probability `sparsity` and 1 otherwise.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    generate_values() -> torch.Tensor\n",
    "        Generate the values for the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def generate_values(self):\n",
    "        \"\"\"\n",
    "        Generate the values for the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The values for the dataset, with indices that are 1.\n",
    "        \"\"\"\n",
    "        return 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_convex_hull_vertices(W: torch.Tensor) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the number of vertices of the convex hull of the points represented by the columns of W.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : torch.Tensor\n",
    "        A 2xN matrix where each column represents a point in 2D space.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of vertices of the convex hull.\n",
    "    \"\"\"\n",
    "    if W.shape[0] != 2:\n",
    "        raise ValueError(\"The weight matrix W must have 2 rows.\")\n",
    "    \n",
    "    # Convert the tensor to a numpy array if it isn't already\n",
    "    if isinstance(W, torch.Tensor):\n",
    "        W = W.cpu().detach().numpy()\n",
    "    \n",
    "    hull = ConvexHull(W.T)\n",
    "    return len(hull.vertices)  # The number of vertices is the same as the number of edges\n",
    "\n",
    "def count_kgons(W: List[Dict[str, Any]]) -> Dict[int, int]:\n",
    "    \"\"\"\n",
    "    Count the number of k-gons (polygons with k edges) in the list of weight matrices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : list of dict\n",
    "        A list of dictionaries, each containing weight matrices.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where the keys are the number of edges and the values are the counts of those k-gons.\n",
    "    \"\"\"\n",
    "    edge_counts = {}\n",
    "    \n",
    "    # Process each weight matrix\n",
    "    for full_w in W:\n",
    "        num_edges = classify_kgon(full_w)\n",
    "        if num_edges in edge_counts:\n",
    "            edge_counts[num_edges] += 1\n",
    "        else:\n",
    "            edge_counts[num_edges] = 1\n",
    "\n",
    "    return edge_counts\n",
    "\n",
    "def classify_5_gon(W: np.ndarray, b: np.ndarray) -> Any:\n",
    "    \"\"\"\n",
    "    Classify a 5-gon based on the weights and biases.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : np.ndarray\n",
    "        A 2xN or Nx2 array where each row or column represents a point in 2D space.\n",
    "    b : np.ndarray\n",
    "        An array of biases associated with the points.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Any\n",
    "        The classification of the 5-gon. Returns \"not a 5-gon\", 5, or \"5+\" based on the classification criteria.\n",
    "    \"\"\"\n",
    "    if W.shape[0] == 2:\n",
    "        W = W.T\n",
    "\n",
    "    # Compute the convex hull\n",
    "    hull = ConvexHull(W)\n",
    "    \n",
    "    # Check if the number of vertices is equal to 5\n",
    "    if len(hull.vertices) != 5:\n",
    "        return \"not a 5-gon\"\n",
    "    \n",
    "    # Check if any of the non-vertex biases are large negative\n",
    "    non_vertex_biases = np.delete(b, hull.vertices)\n",
    "\n",
    "    # Check for any positive bias that is not part of the convex hull vertices\n",
    "    non_hull_positive_bias = np.any(non_vertex_biases > 0)\n",
    "\n",
    "    if not non_hull_positive_bias:\n",
    "        return 5\n",
    "    elif non_hull_positive_bias:\n",
    "        return \"5+\"\n",
    "    else:\n",
    "        return 'not a 5-gon'\n",
    "\n",
    "def classify_kgon(W: Dict[str, torch.Tensor]) -> int:\n",
    "    \"\"\"\n",
    "    Classify the polygon (k-gon) based on the weight matrix and bias.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : dict\n",
    "        A dictionary containing the weight matrix \"embedding.weight\" and the bias \"unembedding.bias\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of edges (k) of the k-gon.\n",
    "    \"\"\"\n",
    "    embedding_w = W[\"embedding.weight\"]\n",
    "    edges = calculate_convex_hull_vertices(embedding_w)\n",
    "    if edges == 5:\n",
    "        return classify_5_gon(embedding_w, W[\"unembedding.bias\"])\n",
    "    return edges\n",
    "\n",
    "def compute_kgon_percentages(\n",
    "        weights: Dict[int, List[List[Dict[str, torch.Tensor]]]], \n",
    "        steps: List[int], \n",
    "        k_values: List[int] = [5, 6]\n",
    "    ) -> Dict[int, Dict[int, List[float]]]:\n",
    "    \"\"\"\n",
    "    Compute the percentages of k-gons over time for each sparsity and k-value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    weights : dict\n",
    "        A dictionary where keys are sparsities and values are lists of weight matrices for each run.\n",
    "    steps : list of int\n",
    "        A list of time steps at which the weights are recorded.\n",
    "    k_values : list of int, optional\n",
    "        A list of k-values (number of edges) to calculate percentages for, default is [5, 6].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where keys are sparsities and values are dictionaries with k-values as keys and lists of percentages as values.\n",
    "    \"\"\"\n",
    "    sparsities = sorted(weights.keys())\n",
    "    \n",
    "    # Create a dictionary to store percentages of k-gons for each sparsity\n",
    "    kgon_percentages = {sparsity: {k: [] for k in k_values} for sparsity in sparsities}\n",
    "    \n",
    "    # Iterate over each sparsity\n",
    "    for sparsity, runs_weights in weights.items():\n",
    "        # Iterate over each time step\n",
    "        for step_weights in zip(*runs_weights):  # This transposes the list of lists\n",
    "            edge_counts = count_kgons(step_weights)\n",
    "            total_counts = sum(edge_counts.values())\n",
    "            \n",
    "            # Calculate percentages for interested k-gons\n",
    "            for k in k_values:\n",
    "                percentage = (edge_counts.get(k, 0) / total_counts) * 100\n",
    "                kgon_percentages[sparsity][k].append(percentage)\n",
    "        \n",
    "        # Plot the percentage of k-gons over time for each k-value\n",
    "        percentages = np.zeros(len(steps))\n",
    "        for k in k_values:\n",
    "            percentages += kgon_percentages[sparsity][k][:-1]  # fixme: reduced length by one but not sure why this is longer?\n",
    "    return kgon_percentages\n",
    "\n",
    "def plot_percentage_of_kgons_over_time(\n",
    "        weights: Dict[int, List[List[Dict[str, torch.Tensor]]]], \n",
    "        steps: List[int], \n",
    "        k_values: List[int] = [5, 6], \n",
    "        xscales: str = \"log\", \n",
    "        yscales: str = \"linear\", \n",
    "        plot: bool = True, \n",
    "        title: str = None\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Plot the percentage of k-gons over time for different sparsities and k-values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    weights : dict\n",
    "        A dictionary where keys are sparsities and values are lists of weight matrices for each run.\n",
    "    steps : list of int\n",
    "        A list of time steps at which the weights are recorded.\n",
    "    k_values : list of int, optional\n",
    "        A list of k-values (number of edges) to plot percentages for, default is [5, 6].\n",
    "    xscales : str, optional\n",
    "        The scale for the x-axis, default is \"log\".\n",
    "    yscales : str, optional\n",
    "        The scale for the y-axis, default is \"linear\".\n",
    "    plot : bool, optional\n",
    "        Whether to display the plot, default is True.\n",
    "    title : str, optional\n",
    "        Title for the plot, default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Generate a color map to represent different sparsities with a color gradient\n",
    "    sparsities = sorted(weights.keys())\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(sparsities)))\n",
    "    \n",
    "    # Create a dictionary to store percentages of k-gons for each sparsity\n",
    "    kgon_percentages = {sparsity: {k: [] for k in k_values} for sparsity in sparsities}\n",
    "    \n",
    "    # Iterate over each sparsity\n",
    "    for sparsity, runs_weights in weights.items():\n",
    "        # Get the color for the current sparsity\n",
    "        color = colors[sparsities.index(sparsity)]\n",
    "        \n",
    "        # Iterate over each time step\n",
    "        for step_weights in zip(*runs_weights):  # This transposes the list of lists\n",
    "            edge_counts = count_kgons(step_weights)\n",
    "            total_counts = sum(edge_counts.values())\n",
    "            \n",
    "            # Calculate percentages for interested k-gons\n",
    "            for k in k_values:\n",
    "                percentage = (edge_counts.get(k, 0) / total_counts) * 100\n",
    "                kgon_percentages[sparsity][k].append(percentage)\n",
    "        \n",
    "        # Plot the percentage of k-gons over time for each k-value\n",
    "        percentages = np.zeros(len(steps))\n",
    "        for k in k_values:\n",
    "            percentages += kgon_percentages[sparsity][k][:-1]  # fixme: reduced length by one but not sure why this is longer?\n",
    "        label = f'Sparsity: {sparsity}, {\" \".join([str(k) for k in k_values])}-gons'\n",
    "        plt.plot(steps, percentages, label=label, color=color)\n",
    "\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Percentage of k-gons')\n",
    "    plt.xscale(xscales)\n",
    "    plt.yscale(yscales)\n",
    "    if not title:\n",
    "        plt.title(f'Percentage of {\", \".join([str(k) for k in k_values])}-gons over Training Steps for Different Sparsities')\n",
    "    else: \n",
    "        plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "def plot_rate_of_change_of_kgons(\n",
    "        weights: Dict[int, List[List[Dict[str, torch.Tensor]]]], \n",
    "        steps: List[int], \n",
    "        k_values: List[int] = [5, 6], \n",
    "        xscale: str = 'log'\n",
    "        ) -> None:\n",
    "    \"\"\"\n",
    "    Plot the rate of change of the percentage of k-gons over time for different sparsities and k-values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    weights : dict\n",
    "        A dictionary where keys are sparsities and values are lists of weight matrices for each run.\n",
    "    steps : list of int\n",
    "        A list of time steps at which the weights are recorded.\n",
    "    k_values : list of int, optional\n",
    "        A list of k-values (number of edges) to plot the rate of change for, default is [5, 6].\n",
    "    xscale : str, optional\n",
    "        The scale for the x-axis, default is 'log'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    sparsities = sorted(weights.keys())\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(sparsities)))\n",
    "    \n",
    "    kgon_percentages = {sparsity: {k: [] for k in k_values} for sparsity in sparsities}\n",
    "    kgon_rate_of_change = {sparsity: {k: [] for k in k_values} for sparsity in sparsities}\n",
    "        \n",
    "    rate_of_change = np.zeros(len(steps) - 1)\n",
    "    \n",
    "    for sparsity, runs_weights in weights.items():\n",
    "        color = colors[sparsities.index(sparsity)]\n",
    "        \n",
    "        for step_weights in zip(*runs_weights):\n",
    "            edge_counts = count_kgons(step_weights)\n",
    "            total_counts = sum(edge_counts.values())\n",
    "            \n",
    "            for k in k_values:\n",
    "                percentage = (edge_counts.get(k, 0) / total_counts) * 100\n",
    "                kgon_percentages[sparsity][k].append(percentage)\n",
    "        \n",
    "        for k in k_values:\n",
    "            kgon_rate_of_change[sparsity][k] = np.diff(kgon_percentages[sparsity][k])\n",
    "        \n",
    "        for k in k_values:\n",
    "            rate_of_change += kgon_rate_of_change[sparsity][k]\n",
    "        label = f'Sparsity: {sparsity}, {\" \".join([str(k) for k in k_values])}-gons Rate of Change'\n",
    "    plt.plot(steps[1:], rate_of_change, label=label, color=color)\n",
    "    \n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Rate of Change of Percentage of k-gons')\n",
    "    plt.xscale(xscale)\n",
    "    plt.title(f'Rate of Change of {\", \".join([str(k) for k in k_values])}-gons over Training Steps for Different Sparsities')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Iayptf5xV-7"
   },
   "source": [
    "### Environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAei8SlJxV-7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "DEVICE = os.environ.get(\n",
    "    \"DEVICE\",\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\",\n",
    ")\n",
    "DEVICE = torch.device(DEVICE)\n",
    "NUM_CORES = int(os.environ.get(\"NUM_CORES\", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnxEumgPxV-8"
   },
   "source": [
    "### K-gon Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzckAcyZxV-8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_2d_kgon_vertices(k, rot=0., pad_to=None, force_length=0.9):\n",
    "    \"\"\"\n",
    "    Generate the vertices of a 2D k-gon.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        The number of vertices (sides) of the k-gon.\n",
    "    rot : float, optional\n",
    "        Rotation angle in radians, default is 0.\n",
    "    pad_to : Optional[int], optional\n",
    "        If provided, pads the result with zeros to this number of columns, default is None.\n",
    "    force_length : float, optional\n",
    "        The length to scale the vertices to, default is 0.9.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A 2xN array of vertices of the k-gon.\n",
    "    \"\"\"\n",
    "    # Angles for the vertices\n",
    "    theta = np.linspace(0, 2*np.pi, k, endpoint=False) + rot\n",
    "\n",
    "    # Generate the vertices\n",
    "    x = np.cos(theta)\n",
    "    y = np.sin(theta)\n",
    "    result = np.vstack((x, y))\n",
    "\n",
    "    if pad_to is not None and k < pad_to:\n",
    "        num_pad = pad_to - k\n",
    "        result = np.hstack([result, np.zeros((2, num_pad))])\n",
    "\n",
    "    return (result * force_length)\n",
    "\n",
    "def generate_init_param(m, n, init_kgon, length, prior_std=1., no_bias=True, init_zerobias=True, seed=0, force_negb=False, noise=0.01):\n",
    "    \"\"\"\n",
    "    Generate initial parameters for the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m : int\n",
    "        The input dimension.\n",
    "    n : int\n",
    "        The hidden dimension.\n",
    "    init_kgon : Optional[int]\n",
    "        The number of vertices for initializing as a k-gon, default is None.\n",
    "    length : float\n",
    "        The length to scale the k-gon vertices to.\n",
    "    prior_std : float, optional\n",
    "        Standard deviation for the prior distribution, default is 1.0.\n",
    "    no_bias : bool, optional\n",
    "        If True, do not include bias in the parameters, default is True.\n",
    "    init_zerobias : bool, optional\n",
    "        If True, initialize bias to zero, default is True.\n",
    "    seed : int, optional\n",
    "        Random seed for reproducibility, default is 0.\n",
    "    force_negb : bool, optional\n",
    "        If True, force bias to be non-positive, default is False.\n",
    "    noise : float, optional\n",
    "        Standard deviation of noise to add to the k-gon vertices, default is 0.01.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing the initialized weights and optionally bias.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if init_kgon is None or m != 2:\n",
    "        init_W = np.random.normal(size=(m, n)) * prior_std\n",
    "    else:\n",
    "        assert init_kgon <= n\n",
    "        rand_angle = np.random.uniform(0, 2 * np.pi, size=(1,))\n",
    "        noise = np.random.normal(size=(m, n)) * noise\n",
    "        init_W = generate_2d_kgon_vertices(init_kgon, force_length=length, rot=rand_angle, pad_to=n) + noise\n",
    "\n",
    "    if no_bias:\n",
    "        param = {\"W\": init_W}\n",
    "    else:\n",
    "        init_b = np.random.normal(size=(n, 1)) * prior_std\n",
    "        if force_negb:\n",
    "            init_b = -np.abs(init_b)\n",
    "        if init_zerobias:\n",
    "            init_b = init_b * 0\n",
    "        param = {\n",
    "            \"W\": init_W,\n",
    "            \"b\": init_b\n",
    "        }\n",
    "    return param\n",
    "\n",
    "def generate_optimal_solution(m: int, n: int, rot: float = 0.0) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate the optimal solution for a 2D k-gon with specific configuration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m : int\n",
    "        The input dimension, must be 2.\n",
    "    n : int\n",
    "        The number of vertices (sides) of the k-gon, must be 6 for this function.\n",
    "    rot : float, optional\n",
    "        Rotation angle in radians, default is 0.0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing the optimal weights and biases.\n",
    "    \"\"\"\n",
    "    assert m == 2\n",
    "    assert n==6 # Possibly implement other values of n later. See page 46 of dynamical bayseanism paper and code that automatically finds solution(s).\n",
    "    # Solutions exist for multiples of 4 and 5,6 and 7\n",
    "    if n == 6:\n",
    "        l = 1.4142 # confusion: I get as the optimal parameter for the length: 1.4142, but the paper says 1.32053\n",
    "        init_b = - np.ones((n,1)) * 0.9999 # confusion: I get through training, that the optimal bias is -0.9999 instead of -0.61814\n",
    "        \n",
    "\n",
    "    init_w = generate_2d_kgon_vertices(n, rot=rot, force_length=l, pad_to=n)\n",
    "    param = {\n",
    "        \"W\": init_w,\n",
    "        \"b\": init_b\n",
    "    }\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_polygon(\n",
    "    W: torch.Tensor,\n",
    "    b=None,\n",
    "    ax=None,\n",
    "    ax_bias=None,\n",
    "    ax_wnorm=None,\n",
    "    hull_alpha=0.3,\n",
    "    dW=None,\n",
    "    dW_scale=0.3,\n",
    "    orderb=True,\n",
    "    color=\"b\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a polygon formed by the columns of W and optionally the biases and weight norms. Credits: Edmund Lau\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : torch.Tensor\n",
    "        A 2xN or 3xN matrix where each column represents a point in 2D or 3D space.\n",
    "    b : torch.Tensor, optional\n",
    "        Bias tensor to plot, default is None.\n",
    "    ax : matplotlib.axes.Axes, optional\n",
    "        Axes object to plot the polygon, default is None.\n",
    "    ax_bias : matplotlib.axes.Axes, optional\n",
    "        Axes object to plot the biases, default is None.\n",
    "    ax_wnorm : matplotlib.axes.Axes, optional\n",
    "        Axes object to plot the weight norms, default is None.\n",
    "    hull_alpha : float, optional\n",
    "        Alpha transparency for the convex hull, default is 0.3.\n",
    "    dW : torch.Tensor, optional\n",
    "        Changes in weights to plot as arrows, default is None.\n",
    "    dW_scale : float, optional\n",
    "        Scale factor for the weight changes, default is 0.3.\n",
    "    orderb : bool, optional\n",
    "        Whether to order biases, default is True.\n",
    "    color : str, optional\n",
    "        Color for the 3D plot, default is \"b\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.axes.Axes\n",
    "        Axes object with the polygon plotted.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        if W.shape[0] == 2:\n",
    "            fig, ax = plt.subplots(1, 1)\n",
    "        elif W.shape[0] == 3:\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    if W.shape[0] == 2:  # 2D case\n",
    "        # Compute the norms of the columns\n",
    "        norms = np.linalg.norm(W, axis=0)\n",
    "\n",
    "        # Normalize a copy of the vectors for angle calculations\n",
    "        W_normalized = W / norms\n",
    "\n",
    "        # Compute angles from the x-axis for each vector\n",
    "        angles = np.arctan2(W_normalized[1, :], W_normalized[0, :])\n",
    "\n",
    "        # Sort the columns of W by angles\n",
    "        order = np.argsort(angles)\n",
    "        W_sorted = W[:, order]\n",
    "\n",
    "        # Plot the origin\n",
    "        ax.scatter(0, 0, color=\"red\")\n",
    "\n",
    "        # Plot the vectors\n",
    "        for i in range(W_sorted.shape[1]):\n",
    "            ax.quiver(\n",
    "                0,\n",
    "                0,\n",
    "                W_sorted[0, i],\n",
    "                W_sorted[1, i],\n",
    "                angles=\"xy\",\n",
    "                scale_units=\"xy\",\n",
    "                scale=1,\n",
    "                width=0.003,\n",
    "            )\n",
    "        if dW is not None:\n",
    "            dW = -dW_scale * dW / np.max(np.linalg.norm(dW, axis=0))\n",
    "            for col in range(W.shape[1]):\n",
    "                ax.quiver(\n",
    "                    W[0, col],\n",
    "                    W[1, col],\n",
    "                    dW[0, col],\n",
    "                    dW[1, col],\n",
    "                    angles=\"xy\",\n",
    "                    scale_units=\"xy\",\n",
    "                    scale=1,\n",
    "                    color=\"r\",\n",
    "                    width=0.005,\n",
    "                )\n",
    "\n",
    "        # Connect the vectors to form a polygon\n",
    "        polygon = np.column_stack((W_sorted, W_sorted[:, 0]))\n",
    "        ax.plot(polygon[0, :], polygon[1, :], alpha=0.5)\n",
    "\n",
    "        # Plot the convex hull\n",
    "        hull = ConvexHull(W.T)\n",
    "        vs = list(hull.vertices) + [hull.vertices[0]]\n",
    "        ax.plot(W[0, vs], W[1, vs], \"r--\", alpha=hull_alpha)\n",
    "\n",
    "        # Set the aspect ratio of the plot to equal to ensure that angles are displayed correctly\n",
    "        ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    elif W.shape[0] == 3:  # 3D case\n",
    "        # Plot the origin\n",
    "        ax.scatter([0], [0], [0], color=\"red\")\n",
    "\n",
    "        # Plot the vectors\n",
    "        for i in range(W.shape[1]):\n",
    "            ax.plot([0, W[0, i]], [0, W[1, i]], [0, W[2, i]], color)\n",
    "\n",
    "        # Plot the convex hull\n",
    "        hull = ConvexHull(W.T)\n",
    "        for s in hull.simplices:\n",
    "            s = np.append(s, s[0])  # Here we cycle back to the first coordinate\n",
    "            ax.plot(W[0, s], W[1, s], W[2, s], \"r--\", alpha=hull_alpha)\n",
    "    else:\n",
    "        raise ValueError(\"W must have either 2 or 3 rows\")\n",
    "\n",
    "    if b is not None and ax_bias is not None and W.shape[0]==2:\n",
    "        \n",
    "        b_plot = np.ravel(b)\n",
    "        if orderb:\n",
    "            b_plot = b_plot[order]\n",
    "        bar_colors = [\"r\" if val < 0 else \"g\" for val in b_plot]\n",
    "        yticks = np.array(range(1, len(b_plot) + 1))\n",
    "        ax_bias.barh(\n",
    "            yticks - 0.4,\n",
    "            np.abs(b_plot),\n",
    "            height=0.4,\n",
    "            color=bar_colors,\n",
    "            align=\"edge\",\n",
    "        )\n",
    "        ax_bias.set_yticks(yticks)\n",
    "        ax_bias.yaxis.tick_right()\n",
    "        ax_bias.tick_params(axis=\"y\", labelsize=\"x-small\")\n",
    "        ax_bias.tick_params(axis=\"x\", labelsize=\"x-small\")\n",
    "\n",
    "    if ax_wnorm is not None and W.shape[0]==2:\n",
    "        yticks = np.array(range(1, W.shape[1] + 1))\n",
    "        wnorms = np.linalg.norm(W, axis=0)\n",
    "        if orderb:\n",
    "            wnorms = wnorms[order]\n",
    "        ax_wnorm.barh(yticks, width=wnorms, height=0.4, color=\"black\", alpha=0.9, align=\"edge\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_polygons(Ws, biases, axes=None,ax_biases=None):\n",
    "    \"\"\"\n",
    "    Plot multiple polygons with their corresponding biases.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Ws : list of torch.Tensor\n",
    "        List of weight matrices.\n",
    "    biases : list of torch.Tensor\n",
    "        List of bias tensors.\n",
    "    axes : list of matplotlib.axes.Axes, optional\n",
    "        List of axes objects to plot the polygons, default is None.\n",
    "    ax_biases : list of matplotlib.axes.Axes, optional\n",
    "        List of axes objects to plot the biases, default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(1, len(Ws), figsize=(15, 4))\n",
    "    if ax_biases is None:\n",
    "        fig, ax_biases = plt.subplots(1, len(Ws), figsize=(15, 4))\n",
    "\n",
    "    for ax, W,ax_b,b in zip(axes, Ws, ax_biases,biases):\n",
    "        plot_polygon(W,b=b, ax=ax, ax_bias=ax_b,ax_wnorm=ax_b)\n",
    "\n",
    "\n",
    "def plot_losses_and_polygons(steps, losses, highlights, Ws, biases,xscale=\"log\", yscale=\"log\",batch_size=None, run=None, version = None):\n",
    "    \"\"\"\n",
    "    Plot the training loss and snapshots of the weight polygons at specified steps.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    steps : list of int\n",
    "        Training steps at which the losses are recorded.\n",
    "    losses : list of float\n",
    "        Loss values corresponding to the training steps.\n",
    "    highlights : list of int\n",
    "        Steps at which to highlight the weight polygons.\n",
    "    Ws : list of torch.Tensor\n",
    "        List of weight matrices at the highlighted steps.\n",
    "    biases : list of torch.Tensor\n",
    "        List of bias tensors corresponding to the weight matrices.\n",
    "    xscale : str, optional\n",
    "        Scale for the x-axis, default is \"log\".\n",
    "    yscale : str, optional\n",
    "        Scale for the y-axis, default is \"log\".\n",
    "    batch_size : int, optional\n",
    "        Batch size used in training, default is None.\n",
    "    run : int, optional\n",
    "        Run identifier, default is None.\n",
    "    version : str, optional\n",
    "        Version identifier, default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "\n",
    "    gs = fig.add_gridspec(3, len(Ws))\n",
    "    ax_losses = fig.add_subplot(gs[2, :])\n",
    "    ax_polygons = []\n",
    "    ax_biases = []\n",
    "    \n",
    "\n",
    "    max_x, min_x = max([np.max(W[0]) for W in Ws]), min([np.min(W[0]) for W in Ws])\n",
    "    max_y, min_y = max([np.max(W[1]) for W in Ws]), min([np.min(W[1]) for W in Ws])\n",
    "\n",
    "    for i in range(len(Ws)):\n",
    "        ax = fig.add_subplot(gs[0, i], adjustable='box') \n",
    "        ax.set_aspect('equal')\n",
    "        ax_polygons.append(ax)\n",
    "        ax.set_xlim(min_x, max_x)\n",
    "        ax.set_ylim(min_y, max_y+0.5)\n",
    "    for i in range(len(Ws)):\n",
    "        ax = fig.add_subplot(gs[1, i], adjustable='box')\n",
    "        ax_biases.append(ax)\n",
    "        ax.set_xlim(0, 1.5)\n",
    "        #ax.set_ylim(0, 1.5)\n",
    "        \n",
    "        \n",
    "        \n",
    "    ax_losses.plot(steps, losses)\n",
    "    ax_losses.set_xlabel(\"Step\")\n",
    "    ax_losses.set_ylabel(\"Loss\")\n",
    "    ax_losses.set_xscale(xscale)\n",
    "    ax_losses.set_yscale(yscale)\n",
    "\n",
    "    for i, step in enumerate(highlights):\n",
    "        ax_losses.axvline(step, color=\"gray\", linestyle=\"--\")\n",
    "\n",
    "    plot_polygons(Ws,biases,ax_polygons, ax_biases=ax_biases)\n",
    "    version_str = f\"Version: {version}\" if version is not None else \"\"\n",
    "    batch_size_str = f\"Batch size: {batch_size}\" if batch_size is not None else \"\"\n",
    "    run_str = f\"Run: {run}\" if run is not None else \"\"\n",
    "    plt.suptitle(\"Loss and Weight snapshots, \" + batch_size_str + \" \" + run_str+ \" \" + version_str)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHi3V3nmxV-9"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rR7CpkaOxV-9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_and_train(\n",
    "    m: int,\n",
    "    n: int,\n",
    "    num_samples: int,\n",
    "    batch_size: Optional[int] = 1,\n",
    "    num_epochs: int = 100,\n",
    "    sparsity: Union[float, int] = 1,\n",
    "    lr: float = 0.001,\n",
    "    log_ivl: Iterable[int] = [],\n",
    "    device=DEVICE,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0,\n",
    "    init_kgon=None,\n",
    "    no_bias=False,\n",
    "    init_zerobias=False,\n",
    "    length=0.9,\n",
    "    prior_std=10.,\n",
    "    seed=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Create and train a ToyAutoencoder model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m : int\n",
    "        The input dimension.\n",
    "    n : int\n",
    "        The hidden dimension.\n",
    "    num_samples : int\n",
    "        The number of samples in the dataset.\n",
    "    batch_size : Optional[int], optional\n",
    "        The batch size for training, default is 1.\n",
    "    num_epochs : int, optional\n",
    "        The number of epochs to train the model, default is 100.\n",
    "    sparsity : Union[float, int], optional\n",
    "        The sparsity level of the dataset, default is 1.\n",
    "    lr : float, optional\n",
    "        The learning rate for the optimizer, default is 0.001.\n",
    "    log_ivl : Iterable[int], optional\n",
    "        The intervals at which to log training progress, default is an empty list.\n",
    "    device : str, optional\n",
    "        The device to run the training on, default is DEVICE.\n",
    "    momentum : float, optional\n",
    "        The momentum for the optimizer, default is 0.9.\n",
    "    weight_decay : float, optional\n",
    "        The weight decay for the optimizer, default is 0.0.\n",
    "    init_kgon : optional\n",
    "        Initial k-gon configuration, default is None.\n",
    "    no_bias : bool, optional\n",
    "        If True, disables bias in the model, default is False.\n",
    "    init_zerobias : bool, optional\n",
    "        If True, initializes bias to zero, default is False.\n",
    "    length : float, optional\n",
    "        Length parameter for initialization, default is 0.9.\n",
    "    prior_std : float, optional\n",
    "        Standard deviation for prior initialization, default is 10.0.\n",
    "    seed : int, optional\n",
    "        Random seed for reproducibility, default is 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing logs as a DataFrame and weights as a list.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    model = ToyAutoencoder(m, n, final_bias=True)\n",
    "\n",
    "    init_weights = generate_init_param(n, m, init_kgon, length, no_bias=no_bias, init_zerobias=init_zerobias, prior_std=prior_std, seed=seed)\n",
    "    model.embedding.weight.data = torch.from_numpy(init_weights[\"W\"]).float()\n",
    "\n",
    "    if \"b\" in init_weights:\n",
    "        model.unembedding.bias.data = torch.from_numpy(init_weights[\"b\"].flatten()).float()\n",
    "\n",
    "    dataset = SyntheticBinaryValued(num_samples, m, sparsity)\n",
    "    batch_size = batch_size\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    logs = pd.DataFrame([{\"loss\": None, \"acc\": None, \"step\": step} for step in log_ivl])\n",
    "\n",
    "    model.to(device)\n",
    "    weights = []\n",
    "\n",
    "    def log(step):\n",
    "        loss = 0.0\n",
    "        acc = 0.0\n",
    "        length = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = batch.to(device)\n",
    "                outputs = model(batch)\n",
    "                loss += criterion(outputs, batch).item() * len(batch) # adding \"* len(batch)\"\n",
    "                acc += (outputs.round() == batch).float().sum().item()\n",
    "                length += len(batch)\n",
    "\n",
    "        loss /= length\n",
    "        acc /= length\n",
    "\n",
    "        logs.loc[logs[\"step\"] == step, [\"loss\", \"acc\"]] = [loss, acc]\n",
    "        weights.append({k: v.cpu().detach().clone().numpy() for k, v in model.state_dict().items()})\n",
    "\n",
    "    step = 0\n",
    "    log(step)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch)\n",
    "            loss = criterion(outputs, batch)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            if step in log_ivl:\n",
    "                log(step)\n",
    "\n",
    "    return logs, weights\n",
    "\n",
    "def create_and_train_with_optimal_solution(\n",
    "    m: int,\n",
    "    n: int,\n",
    "    num_samples: int,\n",
    "    batch_size: Optional[int] = 1,\n",
    "    num_epochs: int = 100,\n",
    "    sparsity: Union[float, int] = 1,\n",
    "    lr: float = 0.001,\n",
    "    log_ivl: Iterable[int] = [],\n",
    "    device=DEVICE,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0,\n",
    "    init_kgon=None,\n",
    "    no_bias=False,\n",
    "    init_zerobias=False,\n",
    "    prior_std=10.,\n",
    "    seed=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Create and train a ToyAutoencoder model with optimal initial solution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m : int\n",
    "        The input dimension.\n",
    "    n : int\n",
    "        The hidden dimension.\n",
    "    num_samples : int\n",
    "        The number of samples in the dataset.\n",
    "    batch_size : Optional[int], optional\n",
    "        The batch size for training, default is 1.\n",
    "    num_epochs : int, optional\n",
    "        The number of epochs to train the model, default is 100.\n",
    "    sparsity : Union[float, int], optional\n",
    "        The sparsity level of the dataset, default is 1.\n",
    "    lr : float, optional\n",
    "        The learning rate for the optimizer, default is 0.001.\n",
    "    log_ivl : Iterable[int], optional\n",
    "        The intervals at which to log training progress, default is an empty list.\n",
    "    device : str, optional\n",
    "        The device to run the training on, default is DEVICE.\n",
    "    momentum : float, optional\n",
    "        The momentum for the optimizer, default is 0.9.\n",
    "    weight_decay : float, optional\n",
    "        The weight decay for the optimizer, default is 0.0.\n",
    "    init_kgon : optional\n",
    "        Initial k-gon configuration, default is None.\n",
    "    no_bias : bool, optional\n",
    "        If True, disables bias in the model, default is False.\n",
    "    init_zerobias : bool, optional\n",
    "        If True, initializes bias to zero, default is False.\n",
    "    prior_std : float, optional\n",
    "        Standard deviation for prior initialization, default is 10.0.\n",
    "    seed : int, optional\n",
    "        Random seed for reproducibility, default is 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing logs as a DataFrame and weights as a list.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    model = ToyAutoencoder(m, n, final_bias=True)\n",
    "\n",
    "    init_weights = generate_optimal_solution(n,m)\n",
    "    model.embedding.weight.data = torch.from_numpy(init_weights[\"W\"]).float()\n",
    "\n",
    "    if \"b\" in init_weights:\n",
    "        model.unembedding.bias.data = torch.from_numpy(init_weights[\"b\"].flatten()).float()\n",
    "\n",
    "    dataset = SyntheticBinaryValued(num_samples, m, sparsity)\n",
    "    batch_size = batch_size\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    logs = pd.DataFrame([{\"loss\": None, \"acc\": None, \"step\": step} for step in log_ivl])\n",
    "\n",
    "    model.to(device)\n",
    "    weights = []\n",
    "\n",
    "    def log(step):\n",
    "        loss = 0.0\n",
    "        acc = 0.0\n",
    "        length = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = batch.to(device)\n",
    "                outputs = model(batch)\n",
    "                loss += criterion(outputs, batch).item() * len(batch) # adding \"* len(batch)\"\n",
    "                acc += (outputs.round() == batch).float().sum().item()\n",
    "                length += len(batch)\n",
    "\n",
    "        loss /= length\n",
    "        acc /= length\n",
    "\n",
    "        logs.loc[logs[\"step\"] == step, [\"loss\", \"acc\"]] = [loss, acc]\n",
    "        weights.append({k: v.cpu().detach().clone().numpy() for k, v in model.state_dict().items()})\n",
    "\n",
    "    step = 0\n",
    "    log(step)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch)\n",
    "            loss = criterion(outputs, batch)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            if step in log_ivl:\n",
    "                log(step)\n",
    "\n",
    "    return logs, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGoW645FB_4s"
   },
   "source": [
    "## Design of Experiments\n",
    "\n",
    "First, let's list the variables we can play with:\n",
    "- Sparsity\n",
    "- Number of samples\n",
    "- Number of input features\n",
    "- Number of hidden features\n",
    "\n",
    "### Variable ranges\n",
    "\n",
    "Let's now define the ranges we want to explore for each of the variables we're interested in.\n",
    "\n",
    "| Variable | Lower limit | Upper limit | Stepsize | Number of variations |\n",
    "| -------- | ----------- | ----------- | -------- | --- |\n",
    "| Sparsity | 0.0 | 1.0 | 0.25 | 5 |\n",
    "| Number of samples | 100 | 10000 | Log 10 | 3 |\n",
    "| Number of input features | 6 | 14 | 2 | 5 |\n",
    "| Number of hidden features | 2 | 3 | 1 | 2 |\n",
    "\n",
    "Total number of experiments if performing a full factorial: `5 * 3 * 5 * 2 = 150`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sparsity_values(scale, count):\n",
    "    # Generate exponential values from 0 to scale\n",
    "    x = np.linspace(0, scale, count)\n",
    "    # Apply the exponential decay function\n",
    "    values = 1 - np.exp(-x)\n",
    "    return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Akw1d9l4qZJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_dicts = {\n",
    "   \"debug\":\n",
    "{\n",
    "    \"m\": [6],\n",
    "    \"n\": [2],\n",
    "    \"num_samples\": [100], #Later in iteration 2 we will try 1000 samples\n",
    "    \"batch_size\": [1024],\n",
    "    \"num_epochs\": [4500],\n",
    "    \"sparsity\": generate_sparsity_values(5, 10),\n",
    "    \"lr\": [0.005],\n",
    "    \"momentum\": [0.9],\n",
    "    \"weight_decay\": [0.0],\n",
    "    \"init_kgon\": [6],\n",
    "    \"no_bias\": [False],\n",
    "    \"init_zerobias\": [False],\n",
    "    \"prior_std\": [0],\n",
    "    \"seed\": [i for i in range(10)],\n",
    "},\n",
    "    \"1.3.0\": \n",
    "    {\n",
    "    \"m\": [6],\n",
    "    \"n\": [2],\n",
    "    \"num_samples\": [100], #Later in iteration 2 we will try 1000 samples\n",
    "    \"batch_size\": [1024],\n",
    "    \"num_epochs\": [20000],\n",
    "    \"sparsity\": generate_sparsity_values(5, 10),\n",
    "    \"lr\": [0.005],\n",
    "    \"momentum\": [0.9],\n",
    "    \"weight_decay\": [0.0],\n",
    "    \"init_kgon\": [6],\n",
    "    \"no_bias\": [False],\n",
    "    \"init_zerobias\": [False],\n",
    "    \"prior_std\": [0],\n",
    "    \"seed\": [i for i in range(50)],\n",
    "},\n",
    "    \"1.4.0\": \n",
    "    {\n",
    "    \"m\": [6],\n",
    "    \"n\": [2],\n",
    "    \"num_samples\": [10000], #Later in iteration 2 we will try 1000 samples\n",
    "    \"batch_size\": [1024],\n",
    "    \"num_epochs\": [20000],\n",
    "    \"sparsity\": generate_sparsity_values(5, 10),\n",
    "    \"lr\": [0.005],\n",
    "    \"momentum\": [0.9],\n",
    "    \"weight_decay\": [0.0],\n",
    "    \"init_kgon\": [6],\n",
    "    \"no_bias\": [False],\n",
    "    \"init_zerobias\": [False],\n",
    "    \"prior_std\": [0],\n",
    "    \"seed\": [i for i in range(50)],\n",
    "},\n",
    "    \"1.5.0\": \n",
    "    {\n",
    "    \"m\": [6],\n",
    "    \"n\": [2],\n",
    "    \"num_samples\": [100], #Later in iteration 2 we will try 1000 samples\n",
    "    \"batch_size\": [1024],\n",
    "    \"num_epochs\": [20000],\n",
    "    \"sparsity\": generate_sparsity_values(5, 10),\n",
    "    \"lr\": [0.005],\n",
    "    \"momentum\": [0.9],\n",
    "    \"weight_decay\": [0.0],\n",
    "    \"init_kgon\": [6],\n",
    "    \"no_bias\": [False],\n",
    "    \"init_zerobias\": [False],\n",
    "    \"prior_std\": [0],\n",
    "    \"seed\": [i for i in range(50)],\n",
    "},\n",
    "    \"reproduction\": # reproduce the results from the original paper\n",
    "    {\n",
    "    \"m\": [6],\n",
    "    \"n\": [2],\n",
    "    \"num_samples\": [5000],\n",
    "    \"batch_size\": [20],\n",
    "    \"num_epochs\": [4500],\n",
    "    \"sparsity\": [1],\n",
    "    \"lr\": [0.005],\n",
    "    \"momentum\": [0.9],\n",
    "    \"weight_decay\": [0.0],\n",
    "    \"init_kgon\": [4],\n",
    "    \"no_bias\": [False],\n",
    "    \"init_zerobias\": [True], # 4-gon should initialize with zero bias as per table A.1\n",
    "    \"length\": [1.], # 4-gon should initialize with a length of 1 as per table A.1\n",
    "    \"prior_std\": [1.],\n",
    "    \"seed\": [i for i in range(50)],\n",
    "},\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdEeYMHGwx9B"
   },
   "source": [
    "### Train all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_RjPfGTwKQ8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiments(\n",
    "    training_dict: Dict[str, List[Any]],\n",
    "    train_func: Callable[[Dict[str, Any]], Any],\n",
    "    save: bool = False,\n",
    "    file_name: str = None\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Runs experiments for all combinations of parameters in the training dictionary,\n",
    "    with an incremental run_id starting at 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    training_dict : dict\n",
    "        A dictionary where keys are parameter names and values are lists of parameter values.\n",
    "    train_func : callable\n",
    "        A function that takes a dictionary of parameters and returns the result of the training.\n",
    "    save : bool, optional\n",
    "        If True, saves all experiment results to a single file.\n",
    "    file_name : str, optional\n",
    "        Base file name for saving individual experiment results and the combined results file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[dict]\n",
    "        A list of dictionaries, each containing the run_id, parameters used, and the result of the training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract parameter names and their values\n",
    "    param_names = list(training_dict.keys())\n",
    "    param_values = [training_dict[name] for name in param_names]\n",
    "\n",
    "    # Generate all combinations of parameters\n",
    "    combinations = list(itertools.product(*param_values))\n",
    "\n",
    "    # Iterate through each combination\n",
    "    for run_id, combination in enumerate(combinations):\n",
    "        pkl_file_name = file_name + '_' + str(run_id) + '.pkl'\n",
    "        if os.path.exists(pkl_file_name):\n",
    "            continue\n",
    "        params = dict(zip(param_names, combination))\n",
    "       \n",
    "\n",
    "        # Calculate `log_ivl` based on `num_epochs`\n",
    "        num_epochs = params.get('num_epochs', 100)\n",
    "        num_observations = 50  # As per example\n",
    "        steps = sorted(list(set(np.logspace(0, np.log10(num_epochs), num_observations).astype(int))))\n",
    "        params['log_ivl'] = steps\n",
    "        print(f\"starting run {run_id}\")\n",
    "\n",
    "        logs, weights = train_func(**params)\n",
    "        run_result = {\n",
    "            \"run_id\": run_id,\n",
    "            \"parameters\": params,\n",
    "            \"logs\": logs,\n",
    "            \"weights\": weights\n",
    "        }\n",
    "\n",
    "        with open(pkl_file_name, 'wb') as file:\n",
    "            pickle.dump(run_result, file)\n",
    "\n",
    "    all_results = []\n",
    "    for idx in range(len(combinations)):\n",
    "        pkl_file_name = file_name + '_' + str(idx) + '.pkl'\n",
    "        with open(pkl_file_name, 'rb') as file:\n",
    "            all_results.append(pickle.load(file))\n",
    "    if save:\n",
    "        with open(file_name + 'all_runs', 'wb') as file:\n",
    "            pickle.dump(all_results, file)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def plot_experiments(\n",
    "    results: List[Dict[str, Any]],\n",
    "    show:bool = True,\n",
    "    save: bool = False,\n",
    "    file_name: str = None\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Plots the results of the experiments using plot_losses_and_polygons.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : List[dict]\n",
    "        A list of dictionaries, each containing the run_id, parameters used, logs, and weights.\n",
    "    show : bool, optional\n",
    "        If True, displays the plot.\n",
    "    save : bool, optional\n",
    "        If True, saves the plot to a file.\n",
    "    file_name : str, optional\n",
    "        Base file name for saving the plots.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    for result in results:\n",
    "        run_id = result['run_id']\n",
    "        params = result['parameters']\n",
    "        logs = result['logs']\n",
    "        weights = result['weights']\n",
    "\n",
    "        # Extract steps and losses from logs\n",
    "        steps = list(logs['step'].values)\n",
    "        losses = list(logs['loss'].values)\n",
    "\n",
    "        # Generate highlight steps based on the number of epochs\n",
    "        num_epochs = params.get('num_epochs', 100)\n",
    "        num_observations = 50\n",
    "        plot_steps = [min(steps, key=lambda s: abs(s-i)) for i in [0, 200, 2000, 10000, num_epochs - 1]]\n",
    "        plot_indices = [steps.index(s) for s in plot_steps]\n",
    "\n",
    "        # Extract weights at the highlight steps\n",
    "        Ws = [weights[i]['embedding.weight'] for i in plot_indices]\n",
    "\n",
    "        # Plot losses and polygons\n",
    "        plt.figure()\n",
    "        plot_losses_and_polygons(steps, losses, plot_steps, Ws)\n",
    "\n",
    "        # Title the plot based on parameters\n",
    "        keys_in_title = ['run_id','m', 'n' ,'num_samples', 'batch_size', 'sparsity', 'lr']\n",
    "        title = ', '.join(f'{key}: {value}' for key, value in params.items() if key in keys_in_title)\n",
    "        plt.suptitle(f'Run ID: {run_id}\\n {title}')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(f'{file_name}_{run_id}.png')\n",
    "\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpDgD9SIyL4a",
    "outputId": "e95391dc-1fd1-42f9-ec2a-f922fafb9e60",
    "tags": []
   },
   "source": [
    "## Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpDgD9SIyL4a",
    "outputId": "e95391dc-1fd1-42f9-ec2a-f922fafb9e60",
    "tags": []
   },
   "outputs": [],
   "source": [
    "version = \"reproduction\"\n",
    "file_name = f'../data/logs_loss_{version}'\n",
    "results = run_experiments(\n",
    "    training_dicts[version],\n",
    "    create_and_train,\n",
    "    save=True,\n",
    "    file_name=file_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
