{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize reproduction experiments\n",
    "\n",
    "In this notebook we will visualize our attempts to reproduce the results from Chen et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from typing import Iterable, Optional, Callable, Dict, List, Any\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.spatial import ConvexHull\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.nn import functional as F\n",
    "from devinterp.slt import estimate_learning_coeff_with_summary\n",
    "from devinterp.optim.sgld import SGLD\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Models & Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Network class for linear transformation with non-linear activations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        n_instances: int = 1,\n",
    "        tied: bool = True,\n",
    "        final_bias: bool = False,\n",
    "        hidden_bias: bool = False,\n",
    "        nonlinearity: Callable = F.relu,\n",
    "        unit_weights: bool = False,\n",
    "        standard_magnitude: bool = False,\n",
    "        initial_scale_factor: float = 1.0,\n",
    "        initial_bias: Optional[torch.Tensor] = None,\n",
    "        initial_embed: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set the dimensions and parameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_instances = n_instances\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.tied = tied\n",
    "        self.final_bias = final_bias\n",
    "        self.unit_weights = unit_weights\n",
    "        self.standard_magnitude = standard_magnitude\n",
    "\n",
    "        # Define the input layer (embedding)\n",
    "        self.embedding = nn.Linear(self.input_dim, self.hidden_dim, bias=hidden_bias)\n",
    "\n",
    "        # Set initial embeddings if provided\n",
    "        if initial_embed is not None:\n",
    "            self.embedding.weight.data = initial_embed\n",
    "\n",
    "        # Define the output layer (unembedding)\n",
    "        self.unembedding = nn.Linear(self.hidden_dim, self.input_dim, bias=final_bias)\n",
    "\n",
    "        # Set initial bias if provided\n",
    "        if initial_bias is not None:\n",
    "            self.unembedding.bias.data = initial_bias\n",
    "\n",
    "        # If standard magnitude is set, normalize weights and maintain average norm\n",
    "        if self.standard_magnitude:\n",
    "            avg_norm = torch.norm(self.embedding.weight.data, p=2, dim=0).mean()\n",
    "            self.embedding.weight.data = (\n",
    "                F.normalize(self.embedding.weight.data, p=2, dim=0) * avg_norm\n",
    "            )\n",
    "\n",
    "        # If unit weights is set, normalize weights\n",
    "        if self.unit_weights:\n",
    "            self.embedding.weight.data = F.normalize(self.embedding.weight.data, p=2, dim=0)\n",
    "\n",
    "        # Tie the weights of embedding and unembedding layers\n",
    "        if tied:\n",
    "            self.unembedding.weight = torch.nn.Parameter(self.embedding.weight.transpose(0, 1))\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \"\"\"\n",
    "        # Apply the same steps for weights as done during initialization\n",
    "        if self.unit_weights:\n",
    "            self.embedding.weight.data = F.normalize(self.embedding.weight.data, p=2, dim=0)\n",
    "\n",
    "        if self.standard_magnitude:\n",
    "            avg_norm = torch.norm(self.embedding.weight.data, p=2, dim=0).mean()\n",
    "            self.embedding.weight.data = (\n",
    "                F.normalize(self.embedding.weight.data, p=2, dim=0) * avg_norm\n",
    "            )\n",
    "\n",
    "        if self.tied:\n",
    "            self.unembedding.weight.data = self.embedding.weight.data.transpose(0, 1)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = self.unembedding(x)\n",
    "        x = self.nonlinearity(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from [TMS-zoo](https://github.com/JakeMendel/TMS-zoo)\n",
    "\"\"\"\n",
    "\n",
    "from abc import ABC\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SyntheticDataset(Dataset, ABC):\n",
    "    num_samples: int\n",
    "    num_features: int\n",
    "    sparsity: Union[float, int]\n",
    "    # importance: Optional[float]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_samples,\n",
    "        num_features,\n",
    "        sparsity,\n",
    "        # importance=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the  object.\n",
    "\n",
    "        Args:\n",
    "            num_samples: The number of samples to generate.\n",
    "            num_features: The dimension of the feature vector.\n",
    "            sparsity: (float) the probability that a given feature is zero or (int) the number of features that are set to one.\n",
    "            importance: The importance of the features. If None, then the features are weighted uniformly.\n",
    "                        Otherwise, the features are weighted by `importance ** (1 + i)`, where `i` is the index of the feature.\n",
    "        \"\"\"\n",
    "        self.num_samples = num_samples  # The number of samples in the dataset\n",
    "        self.num_features = num_features  # The size of the feature vector for each sample\n",
    "        self.sparsity = sparsity\n",
    "        # self.importance = importance\n",
    "        self.data = self.generate_data()  # Generate the synthetic data\n",
    "\n",
    "    def generate_values(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate_mask(self):\n",
    "        \"\"\"\n",
    "        Generate a sparse mask for the given dataset.\n",
    "\n",
    "        If ``sparsity`` is a float, then the mask is generated by sampling from a Bernoulli distribution with parameter ``1 - sparsity``.\n",
    "        If ``sparsity`` is an integer, then the mask is generated by sampling exactly ``sparsity`` indices without replacement.\n",
    "\n",
    "        Args:\n",
    "            dataset: The dataset to generate the mask for.\n",
    "\n",
    "        Returns:\n",
    "            A sparse mask for the given dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(self.sparsity, float):\n",
    "            return torch.bernoulli(\n",
    "                torch.ones((self.num_samples, self.num_features)) * (1 - self.sparsity)\n",
    "            )\n",
    "        elif isinstance(self.sparsity, int):\n",
    "            mask = torch.zeros((self.num_samples, self.num_features))\n",
    "            for i in range(self.num_samples):\n",
    "                indices = torch.randperm(self.num_features)[: self.sparsity]\n",
    "                mask[i, indices] = 1\n",
    "\n",
    "            return mask\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Sparsity must be a float or an integer. Received {type(self.sparsity)}.\"\n",
    "            )\n",
    "\n",
    "    def generate_data(self):\n",
    "        return self.generate_mask() * self.generate_values()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class SyntheticUniformValued(SyntheticDataset):\n",
    "    \"\"\"\n",
    "    This class creates a synthetic dataset where each sample is a vector which has indices which are zero with probability sparsity and uniform between 0 and 1 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    def generate_values(self):\n",
    "        return torch.rand((self.num_samples, self.num_features))\n",
    "\n",
    "\n",
    "class SyntheticBinaryValued(SyntheticDataset):\n",
    "    \"\"\"\n",
    "    This class creates a synthetic dataset where each sample is a vector which has indices which are zero with probability ``sparsity`` and 1 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    def generate_values(self):\n",
    "        return 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_convex_hull_vertices(W: torch.Tensor) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the number of vertices of the convex hull of the points represented by the columns of W.\n",
    "    \n",
    "    Parameters:\n",
    "    W (torch.Tensor): A 2xN matrix where each column represents a point in 2D space.\n",
    "    \n",
    "    Returns:\n",
    "    int: The number of vertices of the convex hull.\n",
    "    \"\"\"\n",
    "    if W.shape[0] != 2:\n",
    "        raise ValueError(\"The weight matrix W must have 2 rows.\")\n",
    "    \n",
    "    # Convert the tensor to a numpy array if it isn't already\n",
    "    if isinstance(W, torch.Tensor):\n",
    "        W = W.cpu().detach().numpy()\n",
    "    \n",
    "    hull = ConvexHull(W.T)\n",
    "    return len(hull.vertices)  # The number of vertices is the same as the number of edges\n",
    "\n",
    "def count_kgons(W: List[Dict[str, Any]]) -> Dict[int, int]:\n",
    "    edge_counts = {}\n",
    "    \n",
    "    # Process each weight matrix\n",
    "    for full_w in W:\n",
    "        num_edges = classify_kgon(full_w)\n",
    "        if num_edges in edge_counts:\n",
    "            edge_counts[num_edges] += 1\n",
    "        else:\n",
    "            edge_counts[num_edges] = 1\n",
    "\n",
    "    return edge_counts\n",
    "\n",
    "def classify_5_gon(W: np.ndarray, b: np.ndarray) -> Any:\n",
    "    \"\"\"\n",
    "    Classify a 5-gon based on the weights and biases. \n",
    "    \"\"\"\n",
    "    if W.shape[0] == 2:\n",
    "        W = W.T\n",
    "\n",
    "    # Compute the convex hull\n",
    "    hull = ConvexHull(W)\n",
    "    \n",
    "    # Check if the number of vertices is equal to 5\n",
    "    if len(hull.vertices) != 5:\n",
    "        return \"not a 5-gon\"\n",
    "    \n",
    "    # Check if any of the non-vertex biases are large negative\n",
    "    non_vertex_biases = np.delete(b, hull.vertices)\n",
    "\n",
    "    # Check for any positive bias that is not part of the convex hull vertices\n",
    "    non_hull_positive_bias = np.any(non_vertex_biases > 0)\n",
    "\n",
    "    if not non_hull_positive_bias:\n",
    "        return 5\n",
    "    elif non_hull_positive_bias:\n",
    "        return \"5+\"\n",
    "    else:\n",
    "        return 'not a 5-gon'\n",
    "\n",
    "def classify_kgon(W: Dict[str, torch.Tensor]) -> int:\n",
    "    embedding_w = W[\"embedding.weight\"]\n",
    "    edges = calculate_convex_hull_vertices(embedding_w)\n",
    "    if edges == 5:\n",
    "        return classify_5_gon(embedding_w, W[\"unembedding.bias\"])\n",
    "    return edges\n",
    "\n",
    "def compute_kgon_percentages(weights: Dict[int, List[List[Dict[str, torch.Tensor]]]], steps: List[int], k_values: List[int] = [5, 6]) -> Dict[int, Dict[int, List[float]]]:\n",
    "    sparsities = sorted(weights.keys())\n",
    "    \n",
    "    # Create a dictionary to store percentages of k-gons for each sparsity\n",
    "    kgon_percentages = {sparsity: {k: [] for k in k_values} for sparsity in sparsities}\n",
    "    \n",
    "    # Iterate over each sparsity\n",
    "    for sparsity, runs_weights in weights.items():\n",
    "        # Iterate over each time step\n",
    "        for step_weights in zip(*runs_weights):  # This transposes the list of lists\n",
    "            edge_counts = count_kgons(step_weights)\n",
    "            total_counts = sum(edge_counts.values())\n",
    "            \n",
    "            # Calculate percentages for interested k-gons\n",
    "            for k in k_values:\n",
    "                percentage = (edge_counts.get(k, 0) / total_counts) * 100\n",
    "                kgon_percentages[sparsity][k].append(percentage)\n",
    "        \n",
    "        # Plot the percentage of k-gons over time for each k-value\n",
    "        percentages = np.zeros(len(steps))\n",
    "        for k in k_values:\n",
    "            percentages += kgon_percentages[sparsity][k][:-1]  # fixme: reduced length by one but not sure why this is longer?\n",
    "    return kgon_percentages\n",
    "\n",
    "def plot_percentage_of_kgons_over_time(weights: Dict[int, List[List[Dict[str, torch.Tensor]]]], steps: List[int], k_values: List[int] = [5, 6], xscales: str = \"log\", yscales: str = \"linear\", plot: bool = True, title: str = None) -> None:\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Generate a color map to represent different sparsities with a color gradient\n",
    "    sparsities = sorted(weights.keys())\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(sparsities)))\n",
    "    \n",
    "    # Create a dictionary to store percentages of k-gons for each sparsity\n",
    "    kgon_percentages = {sparsity: {k: [] for k in k_values} for sparsity in sparsities}\n",
    "    \n",
    "    # Iterate over each sparsity\n",
    "    for sparsity, runs_weights in weights.items():\n",
    "        # Get the color for the current sparsity\n",
    "        color = colors[sparsities.index(sparsity)]\n",
    "        \n",
    "        # Iterate over each time step\n",
    "        for step_weights in zip(*runs_weights):  # This transposes the list of lists\n",
    "            edge_counts = count_kgons(step_weights)\n",
    "            total_counts = sum(edge_counts.values())\n",
    "            \n",
    "            # Calculate percentages for interested k-gons\n",
    "            for k in k_values:\n",
    "                percentage = (edge_counts.get(k, 0) / total_counts) * 100\n",
    "                kgon_percentages[sparsity][k].append(percentage)\n",
    "        \n",
    "        # Plot the percentage of k-gons over time for each k-value\n",
    "        percentages = np.zeros(len(steps))\n",
    "        for k in k_values:\n",
    "            percentages += kgon_percentages[sparsity][k][:-1]  # fixme: reduced length by one but not sure why this is longer?\n",
    "        label = f'Sparsity: {sparsity}, {\" \".join([str(k) for k in k_values])}-gons'\n",
    "        plt.plot(steps, percentages, label=label, color=color)\n",
    "\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Percentage of k-gons')\n",
    "    plt.xscale(xscales)\n",
    "    plt.yscale(yscales)\n",
    "    if not title:\n",
    "        plt.title(f'Percentage of {\", \".join([str(k) for k in k_values])}-gons over Training Steps for Different Sparsities')\n",
    "    else: \n",
    "        plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "def plot_rate_of_change_of_kgons(weights: Dict[int, List[List[Dict[str, torch.Tensor]]]], steps: List[int], k_values: List[int] = [5, 6], xscale: str = 'log') -> None:\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    sparsities = sorted(weights.keys())\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(sparsities)))\n",
    "    \n",
    "    kgon_percentages = {sparsity: {k: [] for k in k_values} for sparsity in sparsities}\n",
    "    kgon_rate_of_change = {sparsity: {k: [] for k in k_values} for sparsity in sparsities}\n",
    "        \n",
    "    rate_of_change = np.zeros(len(steps) - 1)\n",
    "    \n",
    "    for sparsity, runs_weights in weights.items():\n",
    "        color = colors[sparsities.index(sparsity)]\n",
    "        \n",
    "        for step_weights in zip(*runs_weights):\n",
    "            edge_counts = count_kgons(step_weights)\n",
    "            total_counts = sum(edge_counts.values())\n",
    "            \n",
    "            for k in k_values:\n",
    "                percentage = (edge_counts.get(k, 0) / total_counts) * 100\n",
    "                kgon_percentages[sparsity][k].append(percentage)\n",
    "        \n",
    "        for k in k_values:\n",
    "            kgon_rate_of_change[sparsity][k] = np.diff(kgon_percentages[sparsity][k])\n",
    "        \n",
    "        for k in k_values:\n",
    "            rate_of_change += kgon_rate_of_change[sparsity][k]\n",
    "        label = f'Sparsity: {sparsity}, {\" \".join([str(k) for k in k_values])}-gons Rate of Change'\n",
    "    plt.plot(steps[1:], rate_of_change, label=label, color=color)\n",
    "    \n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Rate of Change of Percentage of k-gons')\n",
    "    plt.xscale(xscale)\n",
    "    plt.title(f'Rate of Change of {\", \".join([str(k) for k in k_values])}-gons over Training Steps for Different Sparsities')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "DEVICE = os.environ.get(\n",
    "    \"DEVICE\",\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\",\n",
    ")\n",
    "DEVICE = torch.device(DEVICE)\n",
    "NUM_CORES = int(os.environ.get(\"NUM_CORES\", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-gon Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2d_kgon_vertices(k, rot=0., pad_to=None, force_length=0.9):\n",
    "    \"\"\"Set the weights of a 2D k-gon to be the vertices of a regular k-gon.\"\"\"\n",
    "    # Angles for the vertices\n",
    "    theta = np.linspace(0, 2*np.pi, k, endpoint=False) + rot\n",
    "\n",
    "    # Generate the vertices\n",
    "    x = np.cos(theta)\n",
    "    y = np.sin(theta)\n",
    "    result = np.vstack((x, y))\n",
    "\n",
    "    if pad_to is not None and k < pad_to:\n",
    "        num_pad = pad_to - k\n",
    "        result = np.hstack([result, np.zeros((2, num_pad))])\n",
    "\n",
    "    return (result * force_length)\n",
    "\n",
    "def generate_init_param(m, n, init_kgon, prior_std=1., no_bias=True, init_zerobias=True, seed=0, force_negb=False, noise=0.01):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if init_kgon is None or m != 2:\n",
    "        init_W = np.random.normal(size=(m, n)) * prior_std\n",
    "    else:\n",
    "        assert init_kgon <= n\n",
    "        rand_angle = np.random.uniform(0, 2 * np.pi, size=(1,))\n",
    "        noise = np.random.normal(size=(m, n)) * noise\n",
    "        init_W = generate_2d_kgon_vertices(init_kgon, rot=rand_angle, pad_to=n) + noise\n",
    "\n",
    "    if no_bias:\n",
    "        param = {\"W\": init_W}\n",
    "    else:\n",
    "        init_b = np.random.normal(size=(n, 1)) * prior_std\n",
    "        if force_negb:\n",
    "            init_b = -np.abs(init_b)\n",
    "        if init_zerobias:\n",
    "            init_b = init_b * 0\n",
    "        param = {\n",
    "            \"W\": init_W,\n",
    "            \"b\": init_b\n",
    "        }\n",
    "    return param\n",
    "\n",
    "def generate_optimal_solution(m,n,rot=0.0):\n",
    "    assert m == 2\n",
    "    assert n==6 # Possibly implement other values of n later. See page 46 of dynamical bayseanism paper and code that automatically finds solution(s).\n",
    "    # Solutions exist for multiples of 4 and 5,6 and 7\n",
    "    if n == 6:\n",
    "        l =1.4142 # confusion: I get as the optimal parameter for the length: 1.4142, but the paper says 1.32053\n",
    "        init_b = - np.ones((n,1)) *0.9999 # confusion: I get through training, that the optimal bias is -0.9999 instead of 0.61814\n",
    "        \n",
    "\n",
    "    init_w = generate_2d_kgon_vertices(n, rot=rot, force_length=l, pad_to=n)\n",
    "    param = {\n",
    "        \"W\": init_w,\n",
    "        \"b\": init_b\n",
    "    }\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polygon(\n",
    "    W: torch.Tensor,\n",
    "    b=None,\n",
    "    ax=None,\n",
    "    ax_bias=None,\n",
    "    ax_wnorm=None,\n",
    "    hull_alpha=0.3,\n",
    "    dW=None,\n",
    "    dW_scale=0.3,\n",
    "    orderb=True,\n",
    "    color=\"b\",\n",
    "):\n",
    "    \"\"\"Credits: Edmund Lau\"\"\"\n",
    "    if ax is None:\n",
    "        if W.shape[0] == 2:\n",
    "            fig, ax = plt.subplots(1, 1)\n",
    "        elif W.shape[0] == 3:\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    if W.shape[0] == 2:  # 2D case\n",
    "        # Compute the norms of the columns\n",
    "        norms = np.linalg.norm(W, axis=0)\n",
    "\n",
    "        # Normalize a copy of the vectors for angle calculations\n",
    "        W_normalized = W / norms\n",
    "\n",
    "        # Compute angles from the x-axis for each vector\n",
    "        angles = np.arctan2(W_normalized[1, :], W_normalized[0, :])\n",
    "\n",
    "        # Sort the columns of W by angles\n",
    "        order = np.argsort(angles)\n",
    "        W_sorted = W[:, order]\n",
    "\n",
    "        # Plot the origin\n",
    "        ax.scatter(0, 0, color=\"red\")\n",
    "\n",
    "        # Plot the vectors\n",
    "        for i in range(W_sorted.shape[1]):\n",
    "            ax.quiver(\n",
    "                0,\n",
    "                0,\n",
    "                W_sorted[0, i],\n",
    "                W_sorted[1, i],\n",
    "                angles=\"xy\",\n",
    "                scale_units=\"xy\",\n",
    "                scale=1,\n",
    "                width=0.003,\n",
    "            )\n",
    "        if dW is not None:\n",
    "            dW = -dW_scale * dW / np.max(np.linalg.norm(dW, axis=0))\n",
    "            for col in range(W.shape[1]):\n",
    "                ax.quiver(\n",
    "                    W[0, col],\n",
    "                    W[1, col],\n",
    "                    dW[0, col],\n",
    "                    dW[1, col],\n",
    "                    angles=\"xy\",\n",
    "                    scale_units=\"xy\",\n",
    "                    scale=1,\n",
    "                    color=\"r\",\n",
    "                    width=0.005,\n",
    "                )\n",
    "\n",
    "        # Connect the vectors to form a polygon\n",
    "        polygon = np.column_stack((W_sorted, W_sorted[:, 0]))\n",
    "        ax.plot(polygon[0, :], polygon[1, :], alpha=0.5)\n",
    "\n",
    "        # Plot the convex hull\n",
    "        hull = ConvexHull(W.T)\n",
    "        vs = list(hull.vertices) + [hull.vertices[0]]\n",
    "        ax.plot(W[0, vs], W[1, vs], \"r--\", alpha=hull_alpha)\n",
    "\n",
    "        # Set the aspect ratio of the plot to equal to ensure that angles are displayed correctly\n",
    "        ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    elif W.shape[0] == 3:  # 3D case\n",
    "        # Plot the origin\n",
    "        ax.scatter([0], [0], [0], color=\"red\")\n",
    "\n",
    "        # Plot the vectors\n",
    "        for i in range(W.shape[1]):\n",
    "            ax.plot([0, W[0, i]], [0, W[1, i]], [0, W[2, i]], color)\n",
    "\n",
    "        # Plot the convex hull\n",
    "        hull = ConvexHull(W.T)\n",
    "        for s in hull.simplices:\n",
    "            s = np.append(s, s[0])  # Here we cycle back to the first coordinate\n",
    "            ax.plot(W[0, s], W[1, s], W[2, s], \"r--\", alpha=hull_alpha)\n",
    "    else:\n",
    "        raise ValueError(\"W must have either 2 or 3 rows\")\n",
    "\n",
    "    if b is not None and ax_bias is not None and W.shape[0]==2:\n",
    "        \n",
    "        b_plot = np.ravel(b)\n",
    "        if orderb:\n",
    "            b_plot = b_plot[order]\n",
    "        bar_colors = [\"r\" if val < 0 else \"g\" for val in b_plot]\n",
    "        yticks = np.array(range(1, len(b_plot) + 1))\n",
    "        ax_bias.barh(\n",
    "            yticks - 0.4,\n",
    "            np.abs(b_plot),\n",
    "            height=0.4,\n",
    "            color=bar_colors,\n",
    "            align=\"edge\",\n",
    "        )\n",
    "        ax_bias.set_yticks(yticks)\n",
    "        ax_bias.yaxis.tick_right()\n",
    "        ax_bias.tick_params(axis=\"y\", labelsize=\"x-small\")\n",
    "        ax_bias.tick_params(axis=\"x\", labelsize=\"x-small\")\n",
    "\n",
    "    if ax_wnorm is not None and W.shape[0]==2:\n",
    "        yticks = np.array(range(1, W.shape[1] + 1))\n",
    "        wnorms = np.linalg.norm(W, axis=0)\n",
    "        if orderb:\n",
    "            wnorms = wnorms[order]\n",
    "        ax_wnorm.barh(yticks, width=wnorms, height=0.4, color=\"black\", alpha=0.9, align=\"edge\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_polygons(Ws, biases, axes=None,ax_biases=None):\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(1, len(Ws), figsize=(15, 4))\n",
    "    if ax_biases is None:\n",
    "        fig, ax_biases = plt.subplots(1, len(Ws), figsize=(15, 4))\n",
    "\n",
    "    for ax, W,ax_b,b in zip(axes, Ws, ax_biases,biases):\n",
    "        plot_polygon(W,b=b, ax=ax, ax_bias=ax_b,ax_wnorm=ax_b)\n",
    "\n",
    "\n",
    "def plot_losses_and_polygons(steps, losses, highlights, Ws, biases,xscale=\"log\", yscale=\"log\",batch_size=None, run=None, version = None):\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "\n",
    "    gs = fig.add_gridspec(3, len(Ws))\n",
    "    ax_losses = fig.add_subplot(gs[2, :])\n",
    "    ax_polygons = []\n",
    "    ax_biases = []\n",
    "    \n",
    "\n",
    "    max_x, min_x = max([np.max(W[0]) for W in Ws]), min([np.min(W[0]) for W in Ws])\n",
    "    max_y, min_y = max([np.max(W[1]) for W in Ws]), min([np.min(W[1]) for W in Ws])\n",
    "\n",
    "    for i in range(len(Ws)):\n",
    "        ax = fig.add_subplot(gs[0, i], adjustable='box') \n",
    "        ax.set_aspect('equal')\n",
    "        ax_polygons.append(ax)\n",
    "        ax.set_xlim(min_x, max_x)\n",
    "        ax.set_ylim(min_y, max_y+0.5)\n",
    "    for i in range(len(Ws)):\n",
    "        ax = fig.add_subplot(gs[1, i], adjustable='box')\n",
    "        ax_biases.append(ax)\n",
    "        ax.set_xlim(0, 1.5)\n",
    "        #ax.set_ylim(0, 1.5)\n",
    "        \n",
    "        \n",
    "        \n",
    "    ax_losses.plot(steps, losses)\n",
    "    ax_losses.set_xlabel(\"Step\")\n",
    "    ax_losses.set_ylabel(\"Loss\")\n",
    "    ax_losses.set_xscale(xscale)\n",
    "    ax_losses.set_yscale(yscale)\n",
    "\n",
    "    for i, step in enumerate(highlights):\n",
    "        ax_losses.axvline(step, color=\"gray\", linestyle=\"--\")\n",
    "\n",
    "    plot_polygons(Ws,biases,ax_polygons, ax_biases=ax_biases)\n",
    "    version_str = f\"Version: {version}\" if version is not None else \"\"\n",
    "    batch_size_str = f\"Batch size: {batch_size}\" if batch_size is not None else \"\"\n",
    "    run_str = f\"Run: {run}\" if run is not None else \"\"\n",
    "    plt.suptitle(\"Loss and Weight snapshots, \" + batch_size_str + \" \" + run_str+ \" \" + version_str)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train(\n",
    "    m: int,\n",
    "    n: int,\n",
    "    num_samples: int,\n",
    "    batch_size: Optional[int] = 1,\n",
    "    num_epochs: int = 100,\n",
    "    sparsity: Union[float, int] = 1,\n",
    "    lr: float = 0.001,\n",
    "    log_ivl: Iterable[int] = [],\n",
    "    device=DEVICE,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0,\n",
    "    init_kgon=None,\n",
    "    no_bias=False,\n",
    "    init_zerobias=False,\n",
    "    prior_std=10.,\n",
    "    seed=0\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    model = ToyAutoencoder(m, n, final_bias=True)\n",
    "\n",
    "    init_weights = generate_optimal_solution(n,m)\n",
    "    model.embedding.weight.data = torch.from_numpy(init_weights[\"W\"]).float()\n",
    "\n",
    "    if \"b\" in init_weights:\n",
    "        model.unembedding.bias.data = torch.from_numpy(init_weights[\"b\"].flatten()).float()\n",
    "\n",
    "    dataset = SyntheticBinaryValued(num_samples, m, sparsity)\n",
    "    batch_size = batch_size\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    logs = pd.DataFrame([{\"loss\": None, \"acc\": None, \"step\": step} for step in log_ivl])\n",
    "\n",
    "    model.to(device)\n",
    "    weights = []\n",
    "\n",
    "    def log(step):\n",
    "        loss = 0.0\n",
    "        acc = 0.0\n",
    "        length = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = batch.to(device)\n",
    "                outputs = model(batch)\n",
    "                loss += criterion(outputs, batch).item() * len(batch) # adding \"* len(batch)\"\n",
    "                acc += (outputs.round() == batch).float().sum().item()\n",
    "                length += len(batch)\n",
    "\n",
    "        loss /= length\n",
    "        acc /= length\n",
    "\n",
    "        logs.loc[logs[\"step\"] == step, [\"loss\", \"acc\"]] = [loss, acc]\n",
    "        weights.append({k: v.cpu().detach().clone().numpy() for k, v in model.state_dict().items()})\n",
    "\n",
    "    step = 0\n",
    "    log(step)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch)\n",
    "            loss = criterion(outputs, batch)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            if step in log_ivl:\n",
    "                log(step)\n",
    "\n",
    "    return logs, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(\n",
    "    training_dict: Dict[str, List[Any]],\n",
    "    train_func: Callable[[Dict[str, Any]], Any],\n",
    "    save: bool = False,\n",
    "    file_name: str = None\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Runs experiments for all combinations of parameters in the training dictionary,\n",
    "    with an incremental run_id starting at 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    training_dict : dict\n",
    "        A dictionary where keys are parameter names and values are lists of parameter values.\n",
    "    train_func : callable\n",
    "        A function that takes a dictionary of parameters and returns the result of the training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[dict]\n",
    "        A list of dictionaries, each containing the run_id, parameters used, and the result of the training.\n",
    "    \"\"\"\n",
    "    # Extract parameter names and their values\n",
    "    param_names = list(training_dict.keys())\n",
    "    param_values = [training_dict[name] for name in param_names]\n",
    "\n",
    "    # Generate all combinations of parameters\n",
    "    combinations = list(itertools.product(*param_values))\n",
    "\n",
    "    # Iterate through each combination\n",
    "    for run_id, combination in enumerate(combinations):\n",
    "        pkl_file_name = file_name + '_' + str(run_id) + '.pkl'\n",
    "        if os.path.exists(pkl_file_name):\n",
    "            continue\n",
    "        params = dict(zip(param_names, combination))\n",
    "\n",
    "        # Calculate `log_ivl` based on `num_epochs`\n",
    "        num_epochs = params.get('num_epochs', 100)\n",
    "        num_observations = 50  # As per example\n",
    "        steps = sorted(list(set(np.logspace(0, np.log10(num_epochs), num_observations).astype(int))))\n",
    "        params['log_ivl'] = steps\n",
    "        print(f\"starting run {run_id}\")\n",
    "\n",
    "        logs, weights = train_func(**params)\n",
    "        run_result = {\n",
    "            \"run_id\": run_id,\n",
    "            \"parameters\": params,\n",
    "            \"logs\": logs,\n",
    "            \"weights\": weights\n",
    "        }\n",
    "\n",
    "        with open(pkl_file_name, 'wb') as file:\n",
    "            pickle.dump(run_result, file)\n",
    "\n",
    "    all_results = []\n",
    "    for idx in range(len(combinations)):\n",
    "        pkl_file_name = file_name + '_' + str(idx) + '.pkl'\n",
    "        with open(pkl_file_name, 'rb') as file:\n",
    "            all_results.append(pickle.load(file))\n",
    "    if save:\n",
    "        with open(file_name + 'all_runs', 'wb') as file:\n",
    "            pickle.dump(all_results, file)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def plot_experiments(\n",
    "    results: List[Dict[str, Any]],\n",
    "    show:bool = True,\n",
    "    save: bool = False,\n",
    "    file_name: str = None\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Plots the results of the experiments using plot_losses_and_polygons.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : List[dict]\n",
    "        A list of dictionaries, each containing the run_id, parameters used, logs, and weights.\n",
    "    \"\"\"\n",
    "    for result in results:\n",
    "        run_id = result['run_id']\n",
    "        params = result['parameters']\n",
    "        logs = result['logs']\n",
    "        weights = result['weights']\n",
    "\n",
    "        # Extract steps and losses from logs\n",
    "        steps = list(logs['step'].values)\n",
    "        losses = list(logs['loss'].values)\n",
    "\n",
    "        # Generate highlight steps based on the number of epochs\n",
    "        num_epochs = params.get('num_epochs', 100)\n",
    "        num_observations = 50\n",
    "        plot_steps = [min(steps, key=lambda s: abs(s-i)) for i in [0, 200, 2000, 10000, num_epochs - 1]]\n",
    "        plot_indices = [steps.index(s) for s in plot_steps]\n",
    "\n",
    "        # Extract weights at the highlight steps\n",
    "        Ws = [weights[i]['embedding.weight'] for i in plot_indices]\n",
    "\n",
    "        # Plot losses and polygons\n",
    "        plt.figure()\n",
    "        plot_losses_and_polygons(steps, losses, plot_steps, Ws)\n",
    "\n",
    "        # Title the plot based on parameters\n",
    "        keys_in_title = ['run_id','m', 'n' ,'num_samples', 'batch_size', 'sparsity', 'lr']\n",
    "        title = ', '.join(f'{key}: {value}' for key, value in params.items() if key in keys_in_title)\n",
    "        plt.suptitle(f'Run ID: {run_id}\\n {title}')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(f'{file_name}_{run_id}.png')\n",
    "\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(version=\"1.5.0\"):\n",
    "    \n",
    "    file_name = f'../data/logs_loss_{version}'\n",
    "    results = []\n",
    "\n",
    "    for run_id in range(0, 10000000): # This is a hacky way to load all the results\n",
    "        try:\n",
    "            with open(f'{file_name}_{run_id}.pkl', \"rb\") as file:\n",
    "                results.append(pickle.load(file))\n",
    "        except FileNotFoundError as e:\n",
    "            # this is the expected Exception, so just break\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(type(e))\n",
    "            break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot import name 'fastCopyAndTranspose' from 'numpy.core._multiarray_umath' (/Users/andrenogueiraassis/opt/anaconda3/envs/tms/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py)\n",
      "<class 'ImportError'>\n"
     ]
    }
   ],
   "source": [
    "results = load_results(version=\"reproduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
